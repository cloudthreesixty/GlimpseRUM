{
  "AWSTemplateFormatVersion" : "2010-09-09",

  "Description" : "AWS CloudFormation Template to provision GlipseRUM Backend Beacon Collector",

  "Parameters" : {
    "SrcJSURL": {
      "Description" : "GlipseRUM javascript file to be installed in the origin S3 bucket",
      "Type": "String",
      "Default" : "https://github.com/cloudthreesixty/GlimpseRUM/raw/master/rum.js"
    },

    "SrcBeaconURL": {
      "Description" : "GlipseRUM image file to be installed in the origin S3 bucket",
      "Type": "String",
      "Default" : "https://github.com/cloudthreesixty/GlimpseRUM/raw/master/pixel.png"
    },

    "CFdistroFQDN": {
      "Description" : "GlipseRUM cloudfront distribution's full qualified domain name to answer to",
      "Type": "String",
      "Default" : ""
    },

    "CertificateArn": {
      "Description" : "Amazon Resource Name (ARN) of an existing SSL certificate stored in certificate manager for the host defined in CFdistroFQDN",
      "Type": "String",
      "Default" : ""
    }

  },

  "Mappings" : {
  },

  "Resources" : {

	"logsS3Bucket" : {
		"Type": "AWS::S3::Bucket", 
		"Properties": {
		    "AccessControl": "LogDeliveryWrite",
		    "LifecycleConfiguration": {
		      "Rules": [
		        {
		          "Id": "DeleteEverythingInThreeMonths",
		          "Prefix": "",
		          "Status": "Enabled",
		          "ExpirationInDays": "90"
		        }
		      ]
		    },
		    "NotificationConfiguration" : {
		      "LambdaConfigurations" : [{
		        "Function" : { "Fn::GetAtt" : ["CloudFrontLogsProcessFunction", "Arn"] },
		        "Event" : "s3:ObjectCreated:*",
		        "Filter" : {
		          "S3Key" : {
		            "Rules" : [{
		              "Name" : "prefix",
		              "Value" : "cloudfront-logs/"
		            }]
		          }
		        }
		      }]
		    }
		}
	},

	"originS3Bucket" : {
		"Type" : "AWS::S3::Bucket",
		"Properties": {
		    "AccessControl": "PublicRead",
            "LoggingConfiguration": {
                "DestinationBucketName" : { "Ref": "logsS3Bucket" },
                "LogFilePrefix" : "s3-origin-logs/"
            },
		    "WebsiteConfiguration": {
		        "IndexDocument": "index.html",
		        "ErrorDocument": "error.html"
		    },
			"CorsConfiguration" : {
				"CorsRules" : [
					{
						"AllowedHeaders" : [
							"*"
						],
						"AllowedMethods" : [
							"GET", "HEAD", "POST", "PUT"
						],
						"AllowedOrigins" : [
							"*"
						],
						"MaxAge" : "300"
					}
				]
			}
		}
	},


	"dataS3Bucket" : {
		"Type" : "AWS::S3::Bucket"
	},

    "WebsiteWWWCDN": {
        "Type": "AWS::CloudFront::Distribution", 
        "Properties": {
            "DistributionConfig": {
                "Comment": "CDN for S3-backed website", 
                "Aliases": [
                	{"Ref":"CFdistroFQDN"}
                ], 
                "Enabled": "true", 
                "DefaultCacheBehavior": {
                	"Compress" : true,
                    "ForwardedValues": {
                        "QueryString": "true",
                        "Headers": [
                        	"Origin", "Access-Control-Request-Headers", "Access-Control-Request-Method"
                        ]
                    }, 
                    "TargetOriginId": {"Ref":"CFdistroFQDN"}, 
                    "ViewerProtocolPolicy": "redirect-to-https"
                }, 
                "Origins": [
                    {
                        "CustomOriginConfig": {
                            "HTTPPort": "80", 
                            "HTTPSPort": "443", 
                            "OriginProtocolPolicy": "http-only"
                        }, 
                        "DomainName": {
                            "Fn::Join": ["", [{"Ref":"originS3Bucket"}, ".s3-website.", {"Ref": "AWS::Region"}, ".amazonaws.com"]]
                        },
                        "Id":  { "Ref":"CFdistroFQDN"}
                    }
                ],
                "PriceClass": "PriceClass_100",
                "ViewerCertificate" : {
                    "AcmCertificateArn": {"Ref":"CertificateArn"},
                    "SslSupportMethod": "sni-only"
                },
                "Logging": {
                    "IncludeCookies": "true",
                    "Bucket": {
                        "Fn::Join": ["", [{"Ref":"logsS3Bucket"}, ".s3.amazonaws.com"]]
                    },
                    "Prefix": "cloudfront-logs"
                }
            }
        }
    },
    "LambdaResourceDeployExecutionRole": {
        "Type": "AWS::IAM::Role",
        "Properties": {
          "AssumeRolePolicyDocument": {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                      "Service": ["lambda.amazonaws.com"]
                  },
                  "Action": ["sts:AssumeRole"]
                }
              ]
          },
          "Path": "/"
        }
    },

    "LambdaResourceDeployExecutionPolicy": {
        "DependsOn": [
            "LambdaResourceDeployExecutionRole"
        ],
        "Type": "AWS::IAM::Policy",
        "Properties": {
          "PolicyName": "LambdaBillingS3ResourceUpdatePolicy",
          "Roles": [
              {"Ref": "LambdaResourceDeployExecutionRole"}
          ],
          "PolicyDocument": {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Action": ["logs:*"],
                  "Resource": ["arn:aws:logs:*:*:*"]
                },
                {
                  "Effect": "Allow",
                  "Action": [
					"s3:GetObject",
					"s3:GetObjectAcl",
					"s3:PutBucketCORS",
					"s3:PutObject",
					"s3:PutObjectAcl",
					"s3:PutObjectTagging",
					"s3:PutObjectVersionAcl",
					"s3:PutObjectVersionTagging"
                  ],
                  "Resource": [
                    {"Fn::Join" : [ "", [ "arn:aws:s3:::", { "Ref" : "originS3Bucket" } ] ]},
                    {"Fn::Join" : [ "", [ "arn:aws:s3:::", { "Ref" : "originS3Bucket" }, "/*" ] ]}
                  ]
                }
              ]
          }
        }
    },
    "ResourceS3DeployFilesFunction": {
      "DependsOn": [
          "LambdaResourceDeployExecutionRole"
      ],
      "Type" : "AWS::Lambda::Function",
      "Properties" : {
        "Code" : {
          "ZipFile": { "Fn::Join": ["", [
            "from __future__ import print_function\n",
            "import json, urllib.request, boto3\n",
            "import cfnresponse\n",
            "s3 = boto3.resource('s3')\n",
            "bucket = '",{ "Ref" : "originS3Bucket" },"'\n",
            "def lambda_handler(event, context):\n",
            "    print('Received event: ' + json.dumps(event, indent=2))\n",
            "    if event['RequestType'] == 'Create':\n",
            "      print('Create files in bucket: '+bucket)\n",
            "      try:\n",
            "        urls = ['",{ "Ref" : "SrcJSURL" },"', '",{ "Ref" : "SrcBeaconURL" },"']\n",
            "        for url in urls:\n",
            "          print('Downloading file: '+url)\n",
            "          fileSplit = url.split('/')\n",
            "          filename = fileSplit[-1]\n",
            "          fileExtSplit = filename.split('.')\n",
            "          fileExt = fileExtSplit[-1]\n",
            "          contentType = 'text/plain'\n",
            "          if (fileExt == 'js'):\n",
            "            contentType = 'application/javascript'\n",
            "          if (fileExt == 'json'):\n",
            "            contentType = 'application/json'\n",
            "          if (fileExt == 'png'):\n",
            "            contentType = 'image/png'\n",
            "          if (fileExt == 'jpg'):\n",
            "            contentType = 'image/jpeg'\n",
            "          if (fileExt == 'html'):\n",
            "            contentType = 'text/html'\n",
            "          response = urllib.request.urlopen(url)\n",
            "          data = response.read()\n",
            "          print ('got data: '+filename+' sending to bucket: '+bucket)\n",
            "          object = s3.Object(bucket, filename)\n",
            "          object.put(Body=data, ACL='public-read', ContentType=contentType)\n",
            "      except Exception as e:\n",
            "        print(e)\n",
            "        cfnresponse.send(event, context, cfnresponse.FAILURE, {})\n",
            "        raise e\n",
            "    if event['RequestType'] == 'Delete':\n",
            "      print('Delete objects in bucket: '+bucket)\n",
            "      try:\n",
            "        print('Deleting files: ')\n",
            "      except Exception as e:\n",
            "        print(e)\n",
            "        cfnresponse.send(event, context, cfnresponse.FAILURE, {})\n",
            "        raise e\n",
            "    # Signal to CloudFormation endpoint on success\n",
            "    cfnresponse.send(event, context, cfnresponse.SUCCESS, {})\n"
            ]]
          }
        },
        "Description" : "Upload resources from github to S3 bucket",
        "FunctionName" : "ResourceS3DeployFilesFunction",
        "Handler" : "index.lambda_handler",
        "MemorySize" : 128,
        "Role" : { "Fn::GetAtt" : [ "LambdaResourceDeployExecutionRole", "Arn" ]},
        "Runtime" : "python3.6",
        "Timeout" : 30
      }
    },

    "uploadObjectsToS3": {
        "Type": "Custom::uploadObjectsToS3",
        "Properties": {
            "Await": false,
            "ServiceToken": { "Fn::GetAtt" : [ "ResourceS3DeployFilesFunction", "Arn" ]}
        }
    },

    "LambdaS3EventExecutionRole": {
        "Type": "AWS::IAM::Role",
        "Properties": {
            "AssumeRolePolicyDocument": {
                "Version": "2012-10-17",
                "Statement": [
                  {
                    "Effect": "Allow",
                    "Principal": {
                        "Service": ["lambda.amazonaws.com"]
                    },
                    "Action": ["sts:AssumeRole"]
                  }
                ]
            },
            "Path": "/"
        }
    },

    "LambdaEventExecutionPolicy": {
        "DependsOn": [
            "LambdaS3EventExecutionRole"
        ],
        "Type": "AWS::IAM::Policy",
        "Properties": {
          "PolicyName": "LambdaBillingS3",
          "Roles": [
              {"Ref": "LambdaS3EventExecutionRole"}
          ],
          "PolicyDocument": {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Action": ["logs:*"],
                  "Resource": ["arn:aws:logs:*:*:*"]
                },
                {
                  "Effect": "Allow",
                  "Action": [
                      "s3:GetObject"
                  ],
                  "Resource": { "Fn::Join" : [ "", [ "arn:aws:s3:::", { "Ref" : "logsS3Bucket" }, "/*" ] ]}
                },
                {
                  "Effect": "Allow",
                  "Action": [
                    "s3:GetObject",
					"s3:PutObject",
					"s3:PutObjectAcl",
					"s3:PutObjectTagging",
					"s3:PutObjectVersionAcl",
					"s3:PutObjectVersionTagging"
                  ],
                  "Resource": { "Fn::Join" : [ "", [ "arn:aws:s3:::", { "Ref" : "dataS3Bucket" }, "/*" ] ]}
                }
              ]
          }
      }
    },

    "CloudFrontLogsProcessFunction": {
      "DependsOn": [
          "LambdaS3EventExecutionRole"
      ],
      "Type" : "AWS::Lambda::Function",
      "Properties" : {
        "Code" : {
          "ZipFile": { "Fn::Join": ["", [
            "from __future__ import print_function\n",
            "import json, urllib.parse, boto3, gzip, base64\n",
            "\n",
            "print('Loading function')\n",
            "s3 = boto3.resource('s3')\n",
            "def lambda_handler(event, context):\n",
            "    print('Received event: ' + json.dumps(event, indent=2))\n",
            "    for record in event['Records']:\n",
            "        # Get the object from the event and show its content type\n",
            "        bucket = record['s3']['bucket']['name']\n",
            "        key = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')\n",
            "        dstBucket = '",{ "Ref" : "dataS3Bucket" },"'\n",
            "        print ('key: '+key)\n",
            "        try:\n",
            "            s3.Bucket(bucket).download_file(key, '/tmp/my_local.gz')\n",
            "            with gzip.open('/tmp/my_local.gz', 'rb') as f:\n",
            "                file_content = f.readlines()\n",
            "            #print (file_content)\n",
            "            indexFields = []\n",
            "            files_dict = dict()\n",
            "            file_key_map = dict()\n",
            "            for line in file_content:\n",
            "                line = str(line, 'utf-8').rstrip('\\n')\n",
            "                if line.find('#Fields:') == 0:\n",
            "                    # create index\n",
            "                    ndxFields = line.split(' ')\n",
            "                    for ndx in ndxFields:\n",
            "                        if (ndx != '#Fields:'):\n",
            "                            ndx = ndx.replace ('(', '-')\n",
            "                            ndx = ndx.replace (')', '')\n",
            "                            ndx = ndx.replace ('-', '_')\n",
            "                            indexFields.append(ndx.lower())\n",
            "                    #print (json.dumps(indexFields))\n",
            "                if line.find('#') != 0:\n",
            "                    cols = line.split('\t')\n",
            "                    if cols[7] == '/pixel.png':\n",
            "                        outputLine = dict()\n",
            "                        counter = 0\n",
            "                        for col in cols:\n",
            "                            if indexFields[counter] == 'cs_uri_query':\n",
            "                                outputLine[indexFields[counter]] = json.loads(str(base64.b64decode(col), 'utf-8'))\n",
            "                            else:\n",
            "                                outputLine[indexFields[counter]] = col\n",
            "                            counter += 1\n",
            "                        #print (json.dumps(outputLine))\n",
            "                        # Build dst file key\n",
            "                        splitKey = key.split('/')\n",
            "                        timeSplit = outputLine['time'].split(':')\n",
            "                        fileNdx = 'processed_'+outputLine['date']+splitKey[-1]\n",
            "                        dateSplit = outputLine['date'].split('-')\n",
            "                        dstKey = splitKey[0]+'/'+dateSplit[0]+'/'+dateSplit[1]+'/'+dateSplit[2]+'/'+fileNdx\n",
            "                        print ('dstKey: '+dstKey+' fileNdx:'+fileNdx)\n",
            "                        if fileNdx in file_key_map:\n",
            "                            file_key_map[fileNdx]['uncompressed'] += '\\n'+str(json.dumps(outputLine))\n",
            "                        else:\n",
            "                            file_key_map[fileNdx] = dict()\n",
            "                            file_key_map[fileNdx]['key'] = dstKey\n",
            "                            file_key_map[fileNdx]['uncompressed'] = str(json.dumps(outputLine))\n",
            "                        #print ('dstKey: '+dstKey+' fileNdx:'+fileNdx)\n",
            "            for fileNdx in file_key_map:\n",
            "                # write out gzip'd file\n",
            "                output = gzip.open('/tmp/'+fileNdx, 'wb')\n",
            "                try:\n",
            "                    output.write(file_key_map[fileNdx]['uncompressed'].encode())\n",
            "                finally:\n",
            "                    output.close()\n",
            "                print ('write file to bucket: '+dstBucket+' key:'+file_key_map[fileNdx]['key'])\n",
            "                data = open('/tmp/'+fileNdx, 'rb')\n",
            "                s3.Bucket(dstBucket).put_object(Key=file_key_map[fileNdx]['key'], Body=data)\n",
            "                data.close()\n",
            "        except Exception as e:\n",
            "            print(e)\n",
            "            print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))\n",
            "            raise e\n"
            ]]
          }
        },
        "Description" : "Process CloudFront logs into athena compatible",
        "FunctionName" : "CloudFrontLogsProcessFunction",
        "Handler" : "index.lambda_handler",
        "MemorySize" : 512,
        "Role" : { "Fn::GetAtt" : [ "LambdaS3EventExecutionRole", "Arn" ]},
        "Runtime" : "python3.6",
        "Timeout" : 60
      }
    }
  },

  "Outputs" : {
    "CloudFrontDomainNameToCNAME" : {
      "Description" : "CloudFront Domain Name - CNAME to this",
		"Value" : {
			"Fn::GetAtt": [
			    "WebsiteWWWCDN", 
			    "DomainName"
			]
		}
    },
    "S3dataBucketName": {
      "Description" : "S3 bucket where the processed logs will be",
		"Value" : { "Ref": "dataS3Bucket" }
    }
  }
}
